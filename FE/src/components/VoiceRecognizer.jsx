import { useEffect, useRef, useState } from "react";
import { io } from "socket.io-client";
import vad from 'voice-activity-detection';
import axios from "axios";
import './styles/VoiceRecognizer.scss';
import { HelpCircle, AudioLines, MicVocal, HeadphoneOff, Check } from 'lucide-react';

const socket = io("http://localhost:8080");
const VoiceRecognizer = ({
    handlPauseSpeech,
    handleResumeSpeech,
    handleReadingCurrentPage,
    handleStopSpeech,
    textChunks,
}) => {
    const [text, setText] = useState("ƒêang k·∫øt n·ªëi...");
    const [isRecording, setIsRecording] = useState(false);
    const [error, setError] = useState("");
    const [isProcessing, setIsProcessing] = useState(false);
    const [showDots, setShowDots] = useState(false);

    const audioContextRef = useRef(null);
    const vadCleanupRef = useRef(null);
    const streamRef = useRef(null);
    const startVADRef = useRef(false);
    const recognitionRef = useRef(null);         // D√πng ƒë·ªÉ gi·ªØ instance c·ªßa SpeechRecognition
    const isRecognizingRef = useRef(false);      // D√πng ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i ƒëang nh·∫≠n di·ªán
    const textReadingRef = useRef([]);

    useEffect(() => {
        textReadingRef.current = textChunks;
    }, [textChunks]);

    const setupVAD = async () => {
        if (startVADRef.current) {
            console.log('‚úÖ ƒê√£ kh·ªüi t·∫°o VAD');
            if (audioContextRef.current || streamRef.current) {
                console.warn('‚ö†Ô∏è VAD ƒë√£ ƒë∆∞·ª£c b·∫≠t. Kh√¥ng kh·ªüi t·∫°o l·∫°i.');
                return;
            }

            try {
                // Kh·ªüi t·∫°o stream micro v·ªõi c·∫•u h√¨nh gi·∫£m nhi·ªÖu
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        sampleRate: 44100,
                    },
                });
                streamRef.current = stream;

                // Kh·ªüi t·∫°o AudioContext
                const AudioContext = window.AudioContext || window.webkitAudioContext;
                const audioContext = new AudioContext();
                audioContextRef.current = audioContext;

                // C·∫•u h√¨nh VAD
                const options = {
                    onVoiceStart: () => {
                        console.log('üé§ Ph√°t hi·ªán ƒëang n√≥i...');
                        startSpeechRecognition(stream); // Truy·ªÅn stream v√†o
                    },
                    onVoiceStop: () => {
                        console.log('üîá D·ª´ng n√≥i...');
                        if (recognitionRef.current) {
                            recognitionRef.current.stop();
                        }
                    },
                    interval: 100,
                    energyOffset: 1.0,
                };

                const stopVAD = vad(audioContext, stream, options);
                if (typeof stopVAD === 'function') {
                    vadCleanupRef.current = stopVAD;
                } else {
                    console.warn('‚ö†Ô∏è VAD kh√¥ng tr·∫£ v·ªÅ h√†m d·ªçn d·∫πp.');
                }
            } catch (err) {
                console.error('üö´ L·ªói khi kh·ªüi t·∫°o VAD:', err);
                setText('üö´ L·ªói khi kh·ªüi t·∫°o VAD');
            }
        } else {
            console.log('üõë T·∫Øt VAD');
            setText('üõë T·∫Øt VAD');

            // D·ªçn d·∫πp
            if (typeof vadCleanupRef.current === 'function') {
                vadCleanupRef.current();
                vadCleanupRef.current = null;
            }
            if (streamRef.current) {
                streamRef.current.getTracks().forEach((track) => track.stop());
                streamRef.current = null;
            }
            if (audioContextRef.current) {
                audioContextRef.current.close();
                audioContextRef.current = null;
            }
            if (recognitionRef.current) {
                recognitionRef.current.abort();
                recognitionRef.current = null;
            }
            isRecognizingRef.current = false;
        }
    };
    const startSpeechRecognition = (stream) => {
        if (isRecognizingRef.current) {
            console.warn('üîÅ ƒêang nh·∫≠n di·ªán, kh√¥ng kh·ªüi ƒë·ªông l·∫°i.');
            return;
        }

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            console.error('üö´ Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Web Speech API.');
            setText('üö´ Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ Web Speech API.');
            return;
        }

        const recognition = new SpeechRecognition();
        recognition.lang = 'vi-VN';
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;

        isRecognizingRef.current = true;
        recognitionRef.current = recognition;

        console.log('üéôÔ∏è ƒêang l·∫Øng nghe gi·ªçng n√≥i...');

        recognition.onresult = (event) => {
            const transcript = event.results[0][0].transcript.toLowerCase().trim();
            console.log('üó£Ô∏è B·∫°n v·ª´a n√≥i:', transcript);
            setText(`üó£Ô∏è B·∫°n v·ª´a n√≥i: ${transcript}`);

            if (transcript.includes('xin ch√†o')) {
                console.log('‚úÖ ƒê√£ ph√°t hi·ªán c√¢u l·ªánh "xin ch√†o"');
                triggerAPI();
            } else {
                console.log('‚ùå Kh√¥ng kh·ªõp v·ªõi c√¢u l·ªánh y√™u c·∫ßu');
            }
        };

        recognition.onerror = (event) => {
            console.error('‚ùó L·ªói nh·∫≠n di·ªán:', event.error);
            isRecognizingRef.current = false;
            setText(`‚ùó L·ªói nh·∫≠n di·ªán: ${event.error}`);
        };

        recognition.onend = () => {
            console.log('üõë Nh·∫≠n di·ªán k·∫øt th√∫c.');
            isRecognizingRef.current = false;
        };

        recognition.start();
    };

    useEffect(() => {
        const handleToggleVAD = () => {
            setupVAD();
        };

        document.addEventListener("startVAD", handleToggleVAD);
        return () => {
            document.removeEventListener("startVAD", handleToggleVAD);
        };
    }, []);

    useEffect(() => {
        socket.on("connect", () => {
            setText("üü¢ ƒê√£ k·∫øt n·ªëi v·ªõi server. B·∫•m ƒë·ªÉ b·∫Øt ƒë·∫ßu.");
        });

        socket.on("start_recording", (data) => {
            setText(data.message || "üé§ ƒêang ghi √¢m...");
            setIsRecording(true);
        });

        socket.on("result_text", (data) => {
            if (data.status === "success") {
                setText(`‚úÖ ${data.text}`);
                if (data.text.includes("t·∫°m d·ª´ng")) {
                    handlPauseSpeech();
                }
                if (data.text.includes("ti·∫øp t·ª•c")) {
                    handleResumeSpeech();
                }
                if (data.text.includes("b·∫Øt ƒë·∫ßu")) {
                    handleReadingCurrentPage(textReadingRef.current);
                }
                if (data.text.includes("k·∫øt th√∫c")) {
                    handleStopSpeech();
                }
            } else {
                setText(`‚ùå ${data.message || "L·ªói kh√¥ng x√°c ƒë·ªãnh"}`);
            }
            setIsRecording(false);
        });

        socket.on("disconnect", () => {
            setText("üîå M·∫•t k·∫øt n·ªëi v·ªõi server");
        });

        return () => {
            socket.off("start_recording");
            socket.off("result_text");
            socket.off("connect");
            socket.off("disconnect");
        };
    }, []);

    const triggerAPI = async () => {
        if (isProcessing) return;
        setIsProcessing(true);
        try {
            setText("‚è≥ G·ª≠i y√™u c·∫ßu t·ªõi server...");
            const response = await axios.get("http://localhost:8080/api/voice-to-text");
            console.log(response.data.success);
        } catch (err) {
            setError("‚ùå L·ªói g·ªçi API");
            setText("L·ªói khi k·∫øt n·ªëi t·ªõi server.");
        } finally {
            setIsProcessing(false);
        }
    };

    const handleStartVAD = () => {
        startVADRef.current = !startVADRef.current;
        document.dispatchEvent(new Event('startVAD'));
        setShowDots(!showDots);
    };

    const handleShowIcon = () => {
        if (text.includes("üé§")) {
            return <MicVocal className='icon' />
        }
        if (text.includes("‚úÖ")) {
            return <Check className='icon' />
        }
        if (text.includes("‚ùå")) {
            return <HeadphoneOff className='icon' />
        }
        return <AudioLines className='icon' />
    };

    return (
        <button className='support-button'
            onClick={handleStartVAD}
        >
            {showDots ?
                (
                    <>
                        {handleShowIcon()}
                        <div className='dots'>
                            <div className='dot'></div>
                            <div className='dot'></div>
                            <div className='dot'></div>
                        </div>
                    </>
                )
                :
                (
                    <>
                        <HelpCircle className='icon' />
                        <span className='text'>H·ªó tr·ª£</span>
                    </>
                )
            }
        </button>
    );
};

export default VoiceRecognizer;
